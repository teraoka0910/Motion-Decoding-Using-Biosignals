{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pymatreader import read_mat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "data_dir = '../../../data/train'\n",
    "\n",
    "CROP_LEN_ = 250\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_data(data_dir):\n",
    "    \"\"\"\n",
    "    クロスバリデーションのfold分け\n",
    "    ラベルの取得\n",
    "    差分時系列の設定\n",
    "    \"\"\"\n",
    "    data_dict = defaultdict(list)\n",
    "    label_dict = defaultdict(list)\n",
    "\n",
    "    for subject in range(5):\n",
    "        for mat_data in glob.glob(f'{data_dir}/subject{subject}/*'):\n",
    "            data = read_mat(mat_data)\n",
    "            start_indexes = (data['event']['init_time'] + 0.2)*1000 // 2\n",
    "            end_indexes = (data['event']['init_time'] + 0.7)*1000 // 2\n",
    "            labels = data['event']['type']\n",
    "\n",
    "            for start_index, end_index, label in zip(start_indexes, end_indexes, labels):\n",
    "                start_index = int(start_index)\n",
    "                if (subject == 0) or (subject == 3):\n",
    "                    if 'train1' in mat_data:\n",
    "                        data_dict[0].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[0].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train2' in mat_data:\n",
    "                        data_dict[1].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[1].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train3' in mat_data:\n",
    "                        data_dict[2].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[2].append(int(str(int(label))[-1])-1)\n",
    "                elif (subject == 1) or (subject == 4):\n",
    "                    if 'train2' in mat_data:\n",
    "                        data_dict[0].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[0].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train3' in mat_data:\n",
    "                        data_dict[1].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[1].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train1' in mat_data:\n",
    "                        data_dict[2].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[2].append(int(str(int(label))[-1])-1)\n",
    "                elif subject == 2:\n",
    "                    if 'train3' in mat_data:\n",
    "                        data_dict[0].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[0].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train1' in mat_data:\n",
    "                        data_dict[1].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[1].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train2' in mat_data:\n",
    "                        data_dict[2].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[2].append(int(str(int(label))[-1])-1)\n",
    "\n",
    "    \n",
    "    ch_names = [c.replace(' ', '') for c in data['ch_labels']]\n",
    "    diff_list = [\n",
    "        #横方向\n",
    "        'F3_F4',\n",
    "        'FCz_FC1', 'FCz_FC2', 'FCz_FC3', 'FCz_FC4', 'FCz_FC5', 'FCz_FC6', 'FC1_FC2', 'FC3_FC4', 'FC5_FC6',\n",
    "        'Cz_C1', 'Cz_C2', 'Cz_C3', 'Cz_C4', 'Cz_C5', 'Cz_C6', 'C1_C2', 'C3_C4', 'C5_C6',\n",
    "        'CPz_CP1', 'CPz_CP2', 'CPz_CP3', 'CPz_CP4', 'CPz_CP5', 'CPz_CP6', 'CP1_CP2', 'CP3_CP4', 'CP5_CP6',\n",
    "        'P3_P4',\n",
    "        #縦方向\n",
    "        'Cz_FCz', 'C1_FC1', 'C2_FC2', 'C3_FC3', 'C4_FC4', 'C5_FC5', 'C6_FC6',\n",
    "        'Cz_CPz', 'C1_CP1', 'C2_CP2', 'C3_CP3', 'C4_CP4', 'C5_CP5', 'C6_CP6',\n",
    "        'FCz_CPz', 'FC1_CP1', 'FC2_CP2', 'FC3_CP3', 'FC4_CP4', 'FC5_CP5', 'FC6_CP6',\n",
    "    ]\n",
    "\n",
    "    use_ch = []\n",
    "    for item in diff_list:\n",
    "        ch1 = item.split('_')[0]\n",
    "        ch2 = item.split('_')[1]\n",
    "        use_ch.append(ch1)\n",
    "        use_ch.append(ch2)\n",
    "\n",
    "    use_ch = list(set(use_ch))\n",
    "    use_ch_dict = {ch_names[idx]:idx for idx in range(len(ch_names)) if ch_names[idx] in use_ch}\n",
    "    \n",
    "    return data_dict, label_dict, diff_list, use_ch_dict\n",
    "\n",
    "\n",
    "class SkateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    前処理部分\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, label_list, diff_list, use_ch_dict):\n",
    "        self.label_list = label_list\n",
    "        self.diff_list = diff_list\n",
    "        self.use_ch_dict = use_ch_dict\n",
    "        self.crop_len = 250\n",
    "\n",
    "        self.file_list = [item.split('_')[0] for item in data_list]\n",
    "        self.index_list = [item.split('_')[1] for item in data_list]\n",
    "\n",
    "        data_dict = {}\n",
    "        file_name_list = list(set(self.file_list))\n",
    "\n",
    "        for file_name in file_name_list:\n",
    "            data = read_mat(file_name)['data']\n",
    "            data_dict[file_name] = data\n",
    "\n",
    "        data = np.empty((len(self.file_list), 72, 250))\n",
    "        for i, (file_name, idx) in tqdm(enumerate(zip(self.file_list, self.index_list)), total=len(self.file_list)):\n",
    "            idx = int(idx)\n",
    "            eeg_signal = data_dict[file_name][:, idx:idx+self.crop_len]\n",
    "            median = np.median(eeg_signal, axis=1).reshape(72, 1)\n",
    "            q1 = np.percentile(eeg_signal, 25, axis=1)\n",
    "            q3 = np.percentile(eeg_signal, 75, axis=1)\n",
    "            iqr = (q3 - q1).reshape(72, 1)\n",
    "            iqr = np.where(iqr==0, 1, iqr)\n",
    "            eeg_signal = (eeg_signal - median) / iqr\n",
    "            data[i] = eeg_signal\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.label_list[index]\n",
    "        data = self.data[index]\n",
    "        \n",
    "        return data, label\n",
    "    \n",
    "\n",
    "class EEG1DTemporal(nn.Module):\n",
    "    def __init__(self, num_channels=50, num_samples=250):\n",
    "        super(EEG1DTemporal, self).__init__()\n",
    "        #self.conv1 = nn.Conv1d(in_channels, 16, kernel_size=1, stride=1, padding=0)\n",
    "        self.pad = nn.ZeroPad2d((3, 3, 3, 3))\n",
    "        self.conv2d_freq = nn.Conv2d(1, 125, (1, num_samples//2), padding='same', bias=False)\n",
    "        self.bn_freq = nn.BatchNorm2d(125)\n",
    "        self.conv2d_depth = nn.Conv2d(125, 250, (num_channels, 1), groups=125, bias=False, padding='valid')\n",
    "        self.bn_depth = nn.BatchNorm2d(250)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv2d_freq(x)\n",
    "        x = self.bn_freq(x)\n",
    "        x = self.conv2d_depth(x)\n",
    "        x = self.bn_depth(x)\n",
    "        x = self.elu(x)\n",
    "        x = x[:, :, 0, :]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pad(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class EEG2DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEG2DCNN, self).__init__()\n",
    "        self.model_name = 'efficientvit_b1'\n",
    "        self.model = timm.create_model('efficientvit_b1.r256_in1k', pretrained=False, in_chans=1, num_classes=3) #, drop_path_rate=0.1, efficientnetv2_rw_s.ra2_in1k, efficientvit_b3.r256_in1k\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.features = []\n",
    "        if 'tf_efficientnet' in self.model_name:\n",
    "            hook = self.model.global_pool.register_forward_hook(self.hook_fn)\n",
    "        elif 'efficientvit' in self.model_name:\n",
    "            hook = self.model.head.global_pool.register_forward_hook(self.hook_fn)\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x, self.features[0]\n",
    "    \n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features.append(output) \n",
    "    \n",
    "\n",
    "class StakeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StakeModel, self).__init__()\n",
    "        self.oned_encoder = EEG1DTemporal(num_channels=72)\n",
    "        self.twod_encoder = EEG2DCNN()\n",
    "        self.first_dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_dropout(x)\n",
    "        x = self.oned_encoder(x)\n",
    "        x = self.twod_encoder(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "def get_model():\n",
    "    model = StakeModel()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, label_dict, diff_list, use_ch_dict = get_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 795/795 [00:01<00:00, 653.69it/s]\n",
      "100%|██████████| 25/25 [00:01<00:00, 23.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.25 -> 0.88553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 799/799 [00:01<00:00, 647.28it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.05 -> 0.86108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 797/797 [00:01<00:00, 647.92it/s]\n",
      "100%|██████████| 25/25 [00:01<00:00, 23.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.15 -> 0.86700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "best_alpha_list = []\n",
    "for FOLD in range(3):\n",
    "    data = data_dict[FOLD]\n",
    "    label = label_dict[FOLD]\n",
    "    dataset = SkateDataset(data, label, diff_list, use_ch_dict)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = get_model()\n",
    "    load_weights = torch.load(f'model/model{FOLD}.pth', map_location=device)\n",
    "    model.load_state_dict(load_weights)\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    y_pred_list, feature_list, label_list = [], [], []\n",
    "    with tqdm(dataloader) as t:\n",
    "        for i, (X, label) in enumerate(t):\n",
    "            X = X.to(device, non_blocking=True).float()\n",
    "            label = label.to(device, non_blocking=True).long()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred, feature = model(X)\n",
    "            \n",
    "            y_pred_list.append(nn.functional.softmax(y_pred, dim=1))\n",
    "            feature_list.append(feature)\n",
    "            label_list.append(label)\n",
    "\n",
    "    y_pred_list = torch.vstack(y_pred_list)\n",
    "    feature_list = torch.vstack(feature_list)\n",
    "    label_list = torch.hstack(label_list).detach().cpu().numpy()\n",
    "\n",
    "    nn_pred = y_pred_list.detach().cpu().numpy()\n",
    "    lgb_pred = np.load(f'output/lgb_oof_fold{FOLD}.npy', allow_pickle=True)\n",
    "\n",
    "    base_score = 0\n",
    "    best_alpha = -0.1\n",
    "    for alpha in np.arange(0, 1.05, 0.05):\n",
    "        alpha = round(alpha, 3)\n",
    "        preds_= alpha * nn_pred + (1 - alpha) * lgb_pred\n",
    "        preds_ = np.argmax(preds_, axis=1)\n",
    "        accuracy = accuracy_score(label_list, preds_)\n",
    "\n",
    "        if accuracy > base_score:\n",
    "            base_score = accuracy\n",
    "            best_alpha = alpha\n",
    "    print(f'best score: {best_alpha} -> {base_score:.5f}')\n",
    "    best_alpha_list.append(best_alpha)\n",
    "\n",
    "np.save('output/weight', np.array(best_alpha_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
