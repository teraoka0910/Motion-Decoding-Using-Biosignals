{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\teraoka_r\\workspace\\development\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pymatreader import read_mat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "data_dir = '../../../data/train'\n",
    "\n",
    "CROP_LEN_ = 250\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "os.mkdir('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_data(data_dir):\n",
    "    \"\"\"\n",
    "    クロスバリデーションのfold分け\n",
    "    ラベルの取得\n",
    "    差分時系列の設定\n",
    "    \"\"\"\n",
    "    data_dict = defaultdict(list)\n",
    "    label_dict = defaultdict(list)\n",
    "\n",
    "    for subject in range(5):\n",
    "        for mat_data in glob.glob(f'{data_dir}/subject{subject}/*'):\n",
    "            data = read_mat(mat_data)\n",
    "            start_indexes = (data['event']['init_time'] + 0.2)*1000 // 2\n",
    "            end_indexes = (data['event']['init_time'] + 0.7)*1000 // 2\n",
    "            labels = data['event']['type']\n",
    "\n",
    "            for start_index, end_index, label in zip(start_indexes, end_indexes, labels):\n",
    "                start_index = int(start_index)\n",
    "                if (subject == 0) or (subject == 3):\n",
    "                    if 'train1' in mat_data:\n",
    "                        data_dict[0].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[0].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train2' in mat_data:\n",
    "                        data_dict[1].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[1].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train3' in mat_data:\n",
    "                        data_dict[2].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[2].append(int(str(int(label))[-1])-1)\n",
    "                elif (subject == 1) or (subject == 4):\n",
    "                    if 'train2' in mat_data:\n",
    "                        data_dict[0].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[0].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train3' in mat_data:\n",
    "                        data_dict[1].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[1].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train1' in mat_data:\n",
    "                        data_dict[2].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[2].append(int(str(int(label))[-1])-1)\n",
    "                elif subject == 2:\n",
    "                    if 'train3' in mat_data:\n",
    "                        data_dict[0].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[0].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train1' in mat_data:\n",
    "                        data_dict[1].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[1].append(int(str(int(label))[-1])-1)\n",
    "                    elif 'train2' in mat_data:\n",
    "                        data_dict[2].append(f'{mat_data}_{start_index}')\n",
    "                        label_dict[2].append(int(str(int(label))[-1])-1)\n",
    "\n",
    "    \n",
    "    ch_names = [c.replace(' ', '') for c in data['ch_labels']]\n",
    "    diff_list = [\n",
    "        #横方向\n",
    "        'F3_F4',\n",
    "        'FCz_FC1', 'FCz_FC2', 'FCz_FC3', 'FCz_FC4', 'FCz_FC5', 'FCz_FC6', 'FC1_FC2', 'FC3_FC4', 'FC5_FC6',\n",
    "        'Cz_C1', 'Cz_C2', 'Cz_C3', 'Cz_C4', 'Cz_C5', 'Cz_C6', 'C1_C2', 'C3_C4', 'C5_C6',\n",
    "        'CPz_CP1', 'CPz_CP2', 'CPz_CP3', 'CPz_CP4', 'CPz_CP5', 'CPz_CP6', 'CP1_CP2', 'CP3_CP4', 'CP5_CP6',\n",
    "        'P3_P4',\n",
    "        #縦方向\n",
    "        'Cz_FCz', 'C1_FC1', 'C2_FC2', 'C3_FC3', 'C4_FC4', 'C5_FC5', 'C6_FC6',\n",
    "        'Cz_CPz', 'C1_CP1', 'C2_CP2', 'C3_CP3', 'C4_CP4', 'C5_CP5', 'C6_CP6',\n",
    "        'FCz_CPz', 'FC1_CP1', 'FC2_CP2', 'FC3_CP3', 'FC4_CP4', 'FC5_CP5', 'FC6_CP6',\n",
    "    ]\n",
    "\n",
    "    use_ch = []\n",
    "    for item in diff_list:\n",
    "        ch1 = item.split('_')[0]\n",
    "        ch2 = item.split('_')[1]\n",
    "        use_ch.append(ch1)\n",
    "        use_ch.append(ch2)\n",
    "\n",
    "    use_ch = list(set(use_ch))\n",
    "    use_ch_dict = {ch_names[idx]:idx for idx in range(len(ch_names)) if ch_names[idx] in use_ch}\n",
    "    \n",
    "    return data_dict, label_dict, diff_list, use_ch_dict\n",
    "\n",
    "\n",
    "class SkateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    前処理部分\n",
    "    \"\"\"\n",
    "    def __init__(self, fold, data_list, label, diff_list, use_ch_dict):\n",
    "        self.label = label\n",
    "        self.diff_list = diff_list\n",
    "        self.use_ch_dict = use_ch_dict\n",
    "        self.crop_len = 250\n",
    "\n",
    "        self.file_list = [item.split('_')[0] for item in data_list]\n",
    "        self.index_list = [item.split('_')[1] for item in data_list]\n",
    "\n",
    "        self.iqr = np.load(f'../../../data/scaler/iqr{fold}.npy', allow_pickle=True)\n",
    "        self.median = np.load(f'../../../data/scaler/median{fold}.npy', allow_pickle=True)\n",
    "        self.iqr = self.iqr.reshape(72, 1)\n",
    "        self.median = self.median.reshape(72, 1)\n",
    "\n",
    "        self.data_dict = {}\n",
    "        file_name_list = list(set(self.file_list))\n",
    "\n",
    "        for file_name in tqdm(file_name_list):\n",
    "            data = read_mat(file_name)['data']\n",
    "            data = (data - self.median) / self.iqr\n",
    "            eeg_signal = []\n",
    "            for ch_num, channels in enumerate(self.diff_list):\n",
    "                ch1_name = channels.split('_')[0]\n",
    "                ch2_name = channels.split('_')[1]\n",
    "                ch1 = data[self.use_ch_dict[ch1_name]]\n",
    "                ch2 = data[self.use_ch_dict[ch2_name]]\n",
    "                signal = ch1 - ch2\n",
    "                eeg_signal.append(signal)\n",
    "\n",
    "            eeg_signal = np.stack(eeg_signal)\n",
    "            self.data_dict[file_name] = eeg_signal\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.file_list[index]\n",
    "        idx = int(self.index_list[index])\n",
    "        label = self.label[index]\n",
    "\n",
    "        data = self.data_dict[file_name]\n",
    "        eeg_signal = data[:, idx:idx+self.crop_len]\n",
    "        \n",
    "        return eeg_signal, label\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dropout=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.residual = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        self.bn_residual = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        residual = self.bn_residual(residual)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.gelu(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResidualBlock(in_channels, 64),\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 256),\n",
    "            ResidualBlock(256, 512),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_channels=50, num_samples=250):\n",
    "        super(EEGNet, self).__init__()\n",
    "        depth = 5\n",
    "        temporal_filters = 16\n",
    "\n",
    "        self.temporal_conv = nn.Conv2d(1, temporal_filters, (1, num_samples//2), bias=False, padding='same')\n",
    "        self.batch_norm1 = nn.BatchNorm2d(temporal_filters)\n",
    "\n",
    "        self.depthwise_conv = nn.Conv2d(temporal_filters, depth*temporal_filters, (num_channels, 1), groups=temporal_filters, bias=False, padding='valid')\n",
    "        self.batch_norm2 = nn.BatchNorm2d(depth*temporal_filters)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d((1, 4))\n",
    "\n",
    "        self.sep_conv = nn.Conv2d(depth*temporal_filters, depth*temporal_filters*2, (1, num_samples//4), groups=depth*temporal_filters, bias=False, padding='same')\n",
    "        self.batch_norm3 = nn.BatchNorm2d(depth*temporal_filters*2)\n",
    "        self.avg_pool2 = nn.AvgPool2d((1, 8))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.activate = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal_conv(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.sep_conv(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.activate(x) \n",
    "        x = self.avg_pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class SkateModel(nn.Module):\n",
    "    def __init__(self, encoder=None, in_channels=50):\n",
    "        super(SkateModel, self).__init__()\n",
    "        self.encoder1 = ConvEncoder(in_channels)\n",
    "        self.first_dropout = nn.Dropout(0.4)\n",
    "        self.encoder2 = EEGNet()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512+1120, 512, bias=False),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 3, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_dropout(x)\n",
    "        x1 = self.encoder1(x)\n",
    "        x1 = self.pool(x1)\n",
    "        bs, channels, _ = x1.shape\n",
    "        x1 = x1.reshape(bs, channels)\n",
    "\n",
    "        x2 = self.encoder2(x)\n",
    "        x_ = torch.cat([x1, x2], dim=1)\n",
    "        x = self.classifier(x_)\n",
    "\n",
    "        return x, x_\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = SkateModel()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, label_dict, diff_list, use_ch_dict = get_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習部分\n",
    "seed_everything(42)\n",
    "for FOLD in range(3):\n",
    "    data = data_dict[FOLD]\n",
    "    label = label_dict[FOLD]\n",
    "    dataset = SkateDataset(FOLD, data, label, diff_list, use_ch_dict)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = get_model()\n",
    "    load_weights = torch.load(f'model/model{FOLD}.pth', map_location=device)\n",
    "    model.load_state_dict(load_weights)\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    feature_list, label_list = [], []\n",
    "    with tqdm(dataloader) as t:\n",
    "        for i, (X, label) in enumerate(t):\n",
    "            X = X.to(device, non_blocking=True).float()\n",
    "            label = label.to(device, non_blocking=True).long()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred, feature = model(X)\n",
    "            \n",
    "            feature_list.append(feature)\n",
    "            label_list.append(label)\n",
    "\n",
    "    feature_list = torch.vstack(feature_list)\n",
    "    label_list = torch.hstack(label_list)\n",
    "\n",
    "    X = feature_list.detach().cpu().numpy()\n",
    "    y = label_list.detach().cpu().numpy()\n",
    "\n",
    "    data = pd.DataFrame(data=X)\n",
    "    data['label'] = y\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "        data.loc[valid_index, 'fold'] = fold\n",
    "\n",
    "    target = data['label']\n",
    "    use_columns = [i for i in range(X.shape[1])]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'verbose': -1,\n",
    "            'random_state': 42,\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'class_weight': {0: 2.0, 1: 2.0, 2: 1.0},\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 3, 255),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 3, 150),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n",
    "            'subsample_freq': trial.suggest_int('subsample_freq', 0, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-9, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-9, 10.0),\n",
    "        }\n",
    "        \n",
    "        score = list()\n",
    "        \n",
    "        for fold in range(3):\n",
    "            print(f'CV fold: {fold}')\n",
    "            train_idx = data.loc[data.fold != fold].index\n",
    "            valid_idx = data.loc[data.fold == fold].index\n",
    "            X_train, y_train = data[use_columns].iloc[train_idx], target[train_idx]\n",
    "            X_valid, y_valid = data[use_columns].iloc[valid_idx], target[valid_idx]\n",
    "            \n",
    "            model = lgb.LGBMClassifier(\n",
    "                **params, n_estimators=1000, early_stopping_round=20, force_row_wise=True, deterministic=True)\n",
    "            callbacks = [lgb.early_stopping(stopping_rounds=20, verbose=-1)]\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=callbacks)\n",
    "            y_pred = model.predict(X_valid)\n",
    "            accuracy = accuracy_score(y_valid, y_pred)\n",
    "            print(accuracy)\n",
    "\n",
    "            score.append(model.best_score_['valid_0']['multi_logloss']) #\n",
    "        \n",
    "        return np.mean(score)\n",
    "\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler()) #seed=config.random_state\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print('Best Log_Score', study.best_value)\n",
    "    print('Best_params', study.best_params)\n",
    "\n",
    "    params = {\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'objective': 'multiclass',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'multi_logloss',\n",
    "        'class_weight': {0: 2.0, 1: 2.0, 2: 1.0},\n",
    "        'verbosity': 0,\n",
    "        }\n",
    "\n",
    "    params.update(study.best_params)\n",
    "    np.save(f'output/lgb_best_param_fold{FOLD}', params)\n",
    "\n",
    "    oof = np.zeros((len(data), 3))\n",
    "    metric_evaluation = list()\n",
    "\n",
    "    for fold in range(3):\n",
    "        train_idx = data.loc[data.fold != fold].index\n",
    "        valid_idx = data.loc[data.fold == fold].index\n",
    "        X_train, y_train = data[use_columns].iloc[train_idx], target[train_idx]\n",
    "        X_valid, y_valid = data[use_columns].iloc[valid_idx], target[valid_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(\n",
    "            **params, n_estimators=1000, early_stopping_round=20, force_row_wise=True, deterministic=True)\n",
    "        \n",
    "        callbacks = [lgb.early_stopping(stopping_rounds=20, verbose=-1),\n",
    "                    lgb.log_evaluation(period=300, show_stdv=False)]\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=callbacks)\n",
    "        metric_evaluation.append(model.best_score_['valid_0']['multi_logloss'])\n",
    "        oof[valid_idx] = model.predict_proba(X_valid, num_iteration=model.best_iteration_)\n",
    "\n",
    "        pickle.dump(model, open(f'lgb_model/lgb_fold{FOLD}_{fold}', 'wb'))\n",
    "        \n",
    "    print(f'{np.mean(metric_evaluation)}({np.std(metric_evaluation)})')\n",
    "    np.save(f'output/lgb_oof_fold{FOLD}', oof)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
